{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3: Kernels and SVMs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsvalencias/MachineLearning/blob/main/Assignment_3_Kernels_and_SVMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqy47UQRKpSn"
      },
      "source": [
        "# **Assignment 3: Kernels and SVMs, Machine Learning 2021**\n",
        "> *ml-assign3-diagarciaar-lahiguarans-dsvalencias.ipynb*\n",
        "\n",
        "> García Arenas, Diego Alejandro - diagarciaar@unal.edu.co\n",
        "\n",
        "> Higuaran Serrano, Luis Alejandro - lahiguarans@unal.edu.co\n",
        "\n",
        "> Valencia Salazar, Dave Sebastian - dsvalencias@unal.edu.co\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Aj-szsUN2nu"
      },
      "source": [
        "#import list\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX3ghm18KkFB"
      },
      "source": [
        "##1. Train an SVM for detecting whether a word belongs to English or Spanish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxMyKGXuK_6n"
      },
      "source": [
        "### (a) Build training and test data sets. You can use the most frequent words in http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists. Consider words at least 4 characters long and ignore accents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOaf-KyPZvBk"
      },
      "source": [
        "\n",
        "*   English: The initial data set comes from the 10000 most common words in English data set, from the Gutenberg project 2006.\n",
        "*   Spanish: The initial data set comes from the 10000 most common words from the RAE.\n",
        "\n",
        "We deleted special characters in both languages and removed all words with less than four (4) characters before uploading it to Github.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osCGqv7YeNfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9573c896-dada-4aa3-e3af-ee4b8ce8d117"
      },
      "source": [
        "#Setting dataframe from a github URL as pandas object.\n",
        "\n",
        "github_raw_url_dataset = \"https://raw.githubusercontent.com/dsvalencias/MachineLearning/main/Datasets/languages_words.csv\"\n",
        "words_df = pd.read_csv(github_raw_url_dataset)\n",
        "words_df = words_df.sample(frac = 1)\n",
        "print(\"Words dataframe size: %s\" % str(words_df.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words dataframe size: (7000, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5-eRvMjcehd"
      },
      "source": [
        "After finishing data cleansing we end up with 7000 words to implement in all kernels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "1KNcjSkpO0g1",
        "outputId": "471a1547-0ab5-4e42-8a5f-81726cfe950b"
      },
      "source": [
        "#Lowercase language columns\n",
        "words_df['Language'] = words_df['Language'].str.lower()\n",
        "words_df['Language'] = words_df['Language'].str.strip()\n",
        "words_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Word</th>\n",
              "      <th>Length</th>\n",
              "      <th>Language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6548</th>\n",
              "      <td>12503</td>\n",
              "      <td>coste</td>\n",
              "      <td>5</td>\n",
              "      <td>spanish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2717</th>\n",
              "      <td>2719</td>\n",
              "      <td>depth</td>\n",
              "      <td>5</td>\n",
              "      <td>english</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3906</th>\n",
              "      <td>9861</td>\n",
              "      <td>estamos</td>\n",
              "      <td>7</td>\n",
              "      <td>spanish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3021</th>\n",
              "      <td>3023</td>\n",
              "      <td>paragraphs</td>\n",
              "      <td>10</td>\n",
              "      <td>english</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2659</th>\n",
              "      <td>2661</td>\n",
              "      <td>format</td>\n",
              "      <td>6</td>\n",
              "      <td>english</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Index        Word  Length Language\n",
              "6548  12503       coste       5  spanish\n",
              "2717   2719       depth       5  english\n",
              "3906   9861     estamos       7  spanish\n",
              "3021   3023  paragraphs      10  english\n",
              "2659   2661      format       6  english"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iShWLnhP19u",
        "outputId": "997c632d-40d5-4e3f-fbd2-3d81026c480d"
      },
      "source": [
        "#Validation for word length\n",
        "print(\"Unique values for Length column: %s\" % str(words_df.Length.unique()))\n",
        "\n",
        "#Validation for word language\n",
        "print(\"Unique values for Language column: %s\" % str(words_df.Language.unique()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique values for Length column: [ 5  7 10  6  4  9  8 11 12 13 14 15 17 16]\n",
            "Unique values for Language column: ['spanish' 'english']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsHWue4cSCvH"
      },
      "source": [
        "#Word column becomes input\n",
        "X = words_df['Word']\n",
        "#Language column becomes its associated label\n",
        "y = words_df['Language']\n",
        "\n",
        "# Applying <One hot encode> for language\n",
        "Y = list(y)\n",
        "Y = np.array([1 if y == 'english' else 0 for y in Y])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44Acri-RS18W",
        "outputId": "895227a9-2818-46ab-dac0-d55da778d74d"
      },
      "source": [
        "#Split the data with training and testing\n",
        "data_train, data_validation, target_train, target_validation = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
        "\n",
        "print(\"Data train size: %s\\nData validation size: %s\\nTarget train size: %s\\nTarget validation size: %s\\n\" % (str(data_train.shape) , str(data_validation.shape) , str(target_train.shape) , str(target_validation.shape)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data train size: (4690,)\n",
            "Data validation size: (2310,)\n",
            "Target train size: (4690,)\n",
            "Target validation size: (2310,)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0pVtd1LUQ4w",
        "outputId": "ab1d9aa1-f199-4714-d99a-c8643ae025e2"
      },
      "source": [
        "print(\"Data train head\")\n",
        "print(data_train.head())\n",
        "\n",
        "print(\"\\nTarget train head\")\n",
        "print(target_train[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data train head\n",
            "6291    observacion\n",
            "5390    comerciales\n",
            "3888      capacidad\n",
            "556       including\n",
            "1186           thin\n",
            "Name: Word, dtype: object\n",
            "\n",
            "Target train head\n",
            "[0 0 0 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6z1bcIaLSEw"
      },
      "source": [
        "###(b) Implement different string kernels:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRPNPmI0LdTI"
      },
      "source": [
        "####i. Histogram cosine kernel: calculate a bag of n-grams representation (use the CountVectorizer from scikit-learn) and apply the cosine_similarity from scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoTLW06NeOQJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh4eZ6kjLlEH"
      },
      "source": [
        "####ii. Histogram intersection: calculate a bag of n-grams representation, normalize it (the sum of the bins must be equal to 1 ∀i, ||xi||1 = 1.) and calculate the sum of the minimum for each bin of the histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1OohJ8VeO4R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg5r2nQBLzMn"
      },
      "source": [
        "####iii. χ2 kernel: calculate a bag of n-grams representation and apply the chi2_kernel from scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6BPxquCePWJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZHBVcpxL7UX"
      },
      "source": [
        "####iv. SSK kernel: use the code available at this repository https://github.com/helq/python-ssk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLgOhJ-TeP2h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHHv9z9qMHWn"
      },
      "source": [
        "###(c) Use scikit-learn to train different SVMs using precomputed kernels. Use cross validation to find appropriate regularization parameters plotting the training and validation error vs. the regularization parameter. Use a logarithmic scale for C,{2^−15, 2^−14, . . . , 2^10}. Try different configurations of the parameters (in particular different n values for the n-grams)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLs4Bu_HeQcp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqY0JRzBM9u_"
      },
      "source": [
        "###(d) Evaluate the performance of the SVMs in the test data set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yannCld3NGAf"
      },
      "source": [
        "####i. Report the results in a table for the different evaluated configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jXGN2K1eREh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BlGwm5SNKrI"
      },
      "source": [
        "####ii. Illustrate examples of errors (English words mistaken as Spanish, Spanish words mistaken as English). Give a possible explanation for these mistakes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pWC2x9heRep"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8qjdG3ENK0v"
      },
      "source": [
        "####iii. Discuss the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af8nKAbTeSKC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhaxxsH6Kv3u"
      },
      "source": [
        "##2. SVM interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pip1M3pfNduI"
      },
      "source": [
        "###(a) Use the same dataset from question 1 and calculate a bag of n-grams representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l10GGg1xeSwK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyuAYgvBNd04"
      },
      "source": [
        "###(b) Train a SVM using the histogram intersection kernel on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSDxEoujeTGa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG_5cvgyNd6U"
      },
      "source": [
        "###(c) Identify the support vectors found by the SVM training algorithm. Show the samples corresponding to the support vectors with the maximum absolute value of the αi coefficients, for both positive and negative values. Do they make sense? Analyze and discuss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfa3NqUHeTjK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDAuTTDqNd_H"
      },
      "source": [
        "###(d) For different test samples, calculate the classification manually, i.e. compute the kernel between the sample and the support vectors and check how the contribute, positively or negatively, to the final classification. Show those vectors that have the highest value of the kernel. Analyze and discuss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkFw1X-NeazR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrjTMLm8NeEP"
      },
      "source": [
        "###(e) Propose a method that for a given word to be classified, highlight in one color (e.g. blue) those n-grams that suggest the word is from English and in another color (e.g. red) those n-grams that suggest the word is from Spanish.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wtqWnnhebV5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nY6zCWdK3Pu"
      },
      "source": [
        "##3. Kernel logistic regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNVDBfTxK8rf"
      },
      "source": [
        "###(a) Write a expression of the discriminant function expressed in terms of the kernel and the coefficients αi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x2tWYQOeb0B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpsrrtfiN-So"
      },
      "source": [
        "###(b) Formulate the problem of learning the parameters of the model as an optimization problem that looks for the parameters αi that minimize a cross entropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mijGGPCZecJZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pri5gxVkN-WH"
      },
      "source": [
        "###(c) Write a function that receives a training data set and a kernel function and finds a vector α that minimizes the loss function using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mAVq2MCechy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49PvoAZ_N-dn"
      },
      "source": [
        "###(d) Test your algorithm using different kernels (linear, polynomial, Gaussian, etc.) on synthetic 2D datasets from sklearn (https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html). Plot the decision regions and discuss the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNwumSp1ec2i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}